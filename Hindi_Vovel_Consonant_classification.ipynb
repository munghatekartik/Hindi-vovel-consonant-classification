{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hindi_Vovel_Consonant_classification.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIKLavkTN-L6",
        "colab_type": "text"
      },
      "source": [
        "Importing the library required for the project."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNcv0druJxsD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import datasets\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "import os\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_TpC0nKObBt",
        "colab_type": "text"
      },
      "source": [
        "As the lables are in the form 'V5_C5' so we need to convert it into a one hot encoded vector to apply deep learning model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEvTrF_OItZL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "#For converting the dataset to torchvision dataset format\n",
        "class VowelConsonantDataset(Dataset):\n",
        "    def __init__(self, file_path,train=True,transform=None):\n",
        "        self.transform = transform\n",
        "        self.file_path=file_path\n",
        "        self.train=train\n",
        "        self.file_names=[file for _,_,files in os.walk(self.file_path) for file in files]\n",
        "        self.len = len(self.file_names)\n",
        "        if self.train:\n",
        "            self.classes_mapping=self.get_classes()\n",
        "    def __len__(self):\n",
        "        return len(self.file_names)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        file_name=self.file_names[index]\n",
        "        image_data=self.pil_loader(self.file_path+\"/\"+file_name)\n",
        "        if self.transform:\n",
        "            image_data = self.transform(image_data)\n",
        "        if self.train:\n",
        "            file_name_splitted=file_name.split(\"_\")\n",
        "            Y1 = self.classes_mapping[file_name_splitted[0]]\n",
        "            Y2 = self.classes_mapping[file_name_splitted[1]]\n",
        "            z1,z2=torch.zeros(10),torch.zeros(10)\n",
        "            z1[Y1-10],z2[Y2]=1,1\n",
        "            label=torch.stack([z1,z2])\n",
        "\n",
        "            return image_data, label\n",
        "\n",
        "        else:\n",
        "            return image_data, file_name\n",
        "          \n",
        "    def pil_loader(self,path):\n",
        "        with open(path, 'rb') as f:\n",
        "            img = Image.open(f)\n",
        "            return img.convert('RGB')\n",
        "\n",
        "      \n",
        "    def get_classes(self):\n",
        "        classes=[]\n",
        "        for name in self.file_names:\n",
        "            name_splitted=name.split(\"_\")\n",
        "            classes.extend([name_splitted[0],name_splitted[1]])\n",
        "        classes=list(set(classes))\n",
        "        classes_mapping={}\n",
        "        for i,cl in enumerate(sorted(classes)):\n",
        "            classes_mapping[cl]=i\n",
        "        return classes_mapping"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAG9zX-sJ2zi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#function for transforming image to tensor\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyvYz5nEJ86k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Importing the training data and making required changes and transforming the images to tensors\n",
        "full_data = VowelConsonantDataset(\"train.csv\",train=True,transform=transform)\n",
        "train_size = int(0.9 * len(full_data))\n",
        "test_size = len(full_data) - train_size\n",
        "\n",
        "train_data, validation_data = random_split(full_data, [train_size, test_size])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "validation_loader = torch.utils.data.DataLoader(validation_data, batch_size=60, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiR-hnVmKDjP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#printing the length of train loader\n",
        "len(train_loader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYlsJ7p2KFLV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#importing the test data\n",
        "test_data = VowelConsonantDataset(\"test.csv\",train=False,transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=60,shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZlJ577NKHex",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for data in train_loader:\n",
        "    img,lab = data\n",
        "    im = np.transpose(img[0].numpy(),(1,2,0))\n",
        "    \n",
        "    print(im.shape)\n",
        "    im = np.squeeze(im)\n",
        "    print(im.shape)\n",
        "    plt.imshow(im,)\n",
        "   # print(img.shape,lab[0])\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLt-2QP7KKPq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "full_data.get_classes()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKIWxmVmKOu_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "from tqdm import tqdm_notebook"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fjPcD2xKR1p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Initilizing the Resnet18 model for vovels and consonants \n",
        "model_v = models.resnet18()\n",
        "model_c = models.resnet18()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3h9sADnrKT6D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#printing the architecture for Resnet18 model\n",
        "print(model_c)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKLzp4aVNluD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Adding extra dense layer that will give give 10 output classes\n",
        "model_c.fc = nn.Linear(512,10,bias=True)\n",
        "model_v.fc = nn.Linear(512,10,bias=True)\n",
        "print(model_c)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2Xo-QYNNogh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Applying Adam optimizer \n",
        "opt_v = optim.Adam(model_v.parameters())\n",
        "opt_c = optim.Adam(model_c.parameters())\n",
        "\n",
        "#Applyig CrossEntropy loss function \n",
        "loss_fn_v = nn.CrossEntropyLoss()\n",
        "loss_fn_c =  nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irOmp8PdNqq7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(lab[:,0,:],torch.max(lab[:,0,:],1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qq4sk0pENu0n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Iterating over all the images for updating the weights\n",
        "\n",
        "max_epochs = 45\n",
        "loss_arr= []\n",
        "st='cuda:0'\n",
        "model_v.to(st)\n",
        "model_c.to(st)\n",
        "\n",
        "for i in tqdm_notebook(range(max_epochs),total=max_epochs,unit='epochs'):\n",
        "    for data in tqdm_notebook(train_loader,total=len(train_loader),unit='batch'):\n",
        "        img,lab = data\n",
        "        img,lab = img.to(st),lab.to(st)\n",
        "        out_v = model_v(img)\n",
        "        out_c = model_c(img)\n",
        "        \n",
        "        opt_v.zero_grad()\n",
        "        opt_c.zero_grad()\n",
        "        val,ind = torch.max(lab[:,0,:],1)\n",
        "        val,ind1 = torch.max(lab[:,1,:],1)\n",
        "        lab_v = ind\n",
        "        lab_c = ind1\n",
        "        loss = loss_fn_v(out_v,lab_v)+loss_fn_c(out_c,lab_c)\n",
        "        \n",
        "        loss.backward()\n",
        "        opt_v.step()\n",
        "        opt_c.step()\n",
        "        del img,lab\n",
        "        \n",
        "    print(loss)\n",
        "    loss_arr.append(loss)   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaQ455JONzQx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluation(dataloader,m1,m2):\n",
        "    total=0\n",
        "    v=0\n",
        "    c=0\n",
        "    for data in tqdm_notebook(dataloader,total=len(dataloader),unit='batch'):\n",
        "        img,lab = data\n",
        "        img,lab = img.to(st),lab.to(st)\n",
        "        _,out_v = torch.max(m1(img),1)\n",
        "        _,out_c = torch.max(m2(img),1)\n",
        "        _,lab1 = torch.max(lab[:,0,:],1)\n",
        "        _,lab2 = torch.max(lab[:,1,:],1)\n",
        "        total += 64\n",
        "        v += (out_v==lab1).sum().item()\n",
        "        c += (out_c==lab2).sum().item()\n",
        "    print('total images:',total)\n",
        "    print('correct vowels predictions:',v)\n",
        "    print('correct consonants predictions:',c)\n",
        "    print('Vowel Accuracy: ',(v/total)*100, '%')\n",
        "    print('Consonants Accuracy: ',(c/total)*100,'%')\n",
        "        \n",
        "        \n",
        "evaluation(train_loader,model_v,model_c)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzpRB1KxN1W-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "keys = full_data.classes_mapping.keys()\n",
        "keys = list(keys)\n",
        "png = []\n",
        "arr = []\n",
        "model_v.to('cuda:0')\n",
        "model_c.to('cuda:0')\n",
        "for data in tqdm_notebook(test_loader,total=len(test_loader),unit='batch'):\n",
        "    img,lab = data\n",
        "    lab = list(lab)\n",
        "    png.extend(lab)\n",
        "    img = img.to('cuda:0')\n",
        "    out_v = model_v(img)\n",
        "    out_c = model_c(img)\n",
        "    _,ind1 = torch.max(out_v,1)\n",
        "    _,ind2 = torch.max(out_c,1)\n",
        "    for i,j in zip(ind2,ind1):\n",
        "        arr.append(keys[j+10]+'_'+keys[i])    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i65t7e5GN3rt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.DataFrame([png,arr])\n",
        "df = df.transpose()\n",
        "df.columns = ['ImageId','Class']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7unhwnaN5eN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#printing the output class\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9-eOP0lN6RQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#output in the form of csv file\n",
        "df.to_csv('out.csv',index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}